# Hebrew-Dialo-regen
Primary aim of this research project is not necessarily to make the most advanced or precise open-dialog bot, but rather to get an introduction into NLP using Generative pre-training model.


For this project we employed the use of Generative pre-trained 2 (GPT2). GPT2 is an autoregressive language model, which simply means that a given output is taken as an input parameter in the next call. In that sense one could see how such models are self regulating. Below is an illustration of how autoregressiive models operate on a lower level.

https://www.google.com/url?sa=i&url=https%3A%2F%2Feigenfoo.xyz%2Fdeep-autoregressive-models%2F&psig=AOvVaw1iMYi8rtxZF1lByZtqL9Hq&ust=1626200120381000&source=images&cd=vfe&ved=0CAoQjRxqFwoTCLC0-eqR3vECFQAAAAAdAAAAABAD![image](https://user-images.githubusercontent.com/73560826/125336415-bd086c00-e31b-11eb-8387-3e1f1f93c2be.png)



For this project I chose Hebrew as the test language mostly because my first choice laguage did not have a very suitable dataset that could be configured to the desired format (will very likely implement this in. igbo once I'm able to pre-process data effectively).
